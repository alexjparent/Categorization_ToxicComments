{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - RNN + CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "path = \"C:/Users/alex_/Google Drive/Kaggle_ToxicComment/\"\n",
    "sys.path.append(path)\n",
    "\n",
    "EMBEDDING_FILE = 'C:/Users/alex_/Documents/Pre_Trained_Models/word_embeddings/glove/glove.840B.300d.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex_\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(path + \"/data/train_2.csv\",\n",
    "skipinitialspace=True,\n",
    "header = 0,\n",
    "sep = ',',\n",
    "encoding='utf8')\n",
    "\n",
    "test = pd.read_csv(path + \"/data/test_2.csv\",\n",
    "skipinitialspace=True,\n",
    "header = 0,\n",
    "sep = ',',\n",
    "encoding='utf8')\n",
    "\n",
    "train[\"comment_text\"].fillna(\" \")\n",
    "test[\"comment_text\"].fillna(\" \")\n",
    "\n",
    "#X_train = train[\"comment_text\"].str.lower()\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "\n",
    "idx = pd.DataFrame(train[\"id\"]).reset_index(drop=True)\n",
    "idx_train, idx_val = train_test_split(np.asarray(idx.index), train_size=0.85, random_state=17)\n",
    "\n",
    "idx_train = list(idx_train)\n",
    "idx_val = list(idx_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Replace regular expressions <br>\n",
    "* lowrcase <br>\n",
    "* http +  www <br>\n",
    "* \"!\" and \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crap removed\n",
      "only alphabets\n"
     ]
    }
   ],
   "source": [
    "repl = {\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",\n",
    "    \":dd\": \" good \",\n",
    "    \":p\": \" good \",\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":s\": \" bad \",\n",
    "    \":-s\": \" bad \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    r\"\\br\\b\": \"are\",\n",
    "    r\"\\bu\\b\": \"you\",\n",
    "    r\"\\bhaha\\b\": \"ha\",\n",
    "    r\"\\bhahaha\\b\": \"ha\",\n",
    "    r\"\\bdon't\\b\": \"do not\",\n",
    "    r\"\\bdoesn't\\b\": \"does not\",\n",
    "    r\"\\bdidn't\\b\": \"did not\",\n",
    "    r\"\\bhasn't\\b\": \"has not\",\n",
    "    r\"\\bhaven't\\b\": \"have not\",\n",
    "    r\"\\bhadn't\\b\": \"had not\",\n",
    "    r\"\\bwon't\\b\": \"will not\",\n",
    "    r\"\\bwouldn't\\b\": \"would not\",\n",
    "    r\"\\bcan't\\b\": \"can not\",\n",
    "    r\"\\bcannot\\b\": \"can not\",\n",
    "    r\"\\bi'm\\b\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"r\": \"are\",\n",
    "    \"u\": \"you\",\n",
    "    \"haha\": \"ha\",\n",
    "    \"hahaha\": \"ha\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"cannot\": \"can not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"i'll\" : \"i will\",\n",
    "    \"its\" : \"it is\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"'s\" : \" is\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"weren't\" : \"were not\",\n",
    "}\n",
    "\n",
    "keys = [i for i in repl.keys()]\n",
    "\n",
    "new_train_data = []\n",
    "new_test_data = []\n",
    "ltr = train[\"comment_text\"].tolist()\n",
    "lte = test[\"comment_text\"].tolist()\n",
    "for i in ltr:\n",
    "    arr = str(i).split()\n",
    "    xx = \"\"\n",
    "    for j in arr:\n",
    "        j = str(j).lower()\n",
    "        if j[:4] == 'http' or j[:3] == 'www':\n",
    "            continue\n",
    "        if j in keys:\n",
    "            # print(\"inn\")\n",
    "            j = repl[j]\n",
    "        xx += j + \" \"\n",
    "    new_train_data.append(xx)\n",
    "for i in lte:\n",
    "    arr = str(i).split()\n",
    "    xx = \"\"\n",
    "    for j in arr:\n",
    "        j = str(j).lower()\n",
    "        if j[:4] == 'http' or j[:3] == 'www':\n",
    "            continue\n",
    "        if j in keys:\n",
    "            # print(\"inn\")\n",
    "            j = repl[j]\n",
    "        xx += j + \" \"\n",
    "    new_test_data.append(xx)\n",
    "train[\"new_comment_text\"] = new_train_data\n",
    "test[\"new_comment_text\"] = new_test_data\n",
    "print(\"crap removed\")\n",
    "trate = train[\"new_comment_text\"].tolist()\n",
    "tete = test[\"new_comment_text\"].tolist()\n",
    "for i, c in enumerate(trate):\n",
    "    trate[i] = re.sub('[^a-zA-Z ?!]+', '', str(trate[i]).lower())\n",
    "for i, c in enumerate(tete):\n",
    "    tete[i] = re.sub('[^a-zA-Z ?!]+', '', tete[i])\n",
    "train[\"comment_text\"] = trate\n",
    "test[\"comment_text\"] = tete\n",
    "print('only alphabets')\n",
    "\n",
    "\n",
    "embed_size=0\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_t = train[list_classes].values\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"]\n",
    "list_sentences_test = test[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ... functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features=150000\n",
    "maxlen=200\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the input data to the selected parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=text.Tokenizer(num_words=max_features,lower=True)\n",
    "tok.fit_on_texts(list(trate)+list(tete))\n",
    "X_train=tok.texts_to_sequences(trate)\n",
    "X_test=tok.texts_to_sequences(tete)\n",
    "x_train=sequence.pad_sequences(X_train,maxlen=maxlen)\n",
    "x_test=sequence.pad_sequences(X_test,maxlen=maxlen)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float64')\n",
    "        embeddings_index[word] = coefs   \n",
    "        \n",
    "word_index = tok.word_index\n",
    "#prepare embedding matrix\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Word Embeddings + RNN + CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model's artchitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     45000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 200, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 200, 256)     329472      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 198, 64)      49216       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            774         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 45,395,974\n",
      "Trainable params: 395,974\n",
      "Non-trainable params: 45,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(maxlen, ))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "\n",
    "x = Bidirectional(GRU(128, return_sequences=True,dropout=0.15,recurrent_dropout=0.15))(x)\n",
    "conv1 = Conv1D(64, kernel_size = 3, padding = \"valid\")(x)\n",
    "avg_pool_conv1 = GlobalAveragePooling1D()(conv1)\n",
    "max_pool_conv1 = GlobalMaxPooling1D()(conv1)\n",
    "\n",
    "x = concatenate([avg_pool_conv1, max_pool_conv1]) \n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "#xtrain, xval, ytrain, yval = train_test_split(x_train, y_train, train_size=0.85, random_state=17)\n",
    "\n",
    "xtrain = x_train[idx_train]\n",
    "xval = x_train[idx_val]\n",
    "ytrain = y_train[idx_train]\n",
    "yval = y_train[idx_val]\n",
    "\n",
    "filepath = path + \"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "ra_val = RocAucEvaluation(validation_data=(xval, yval), interval = 1)\n",
    "def schedule(ind):\n",
    "    a = [0.0008, 0.0010, 0.0008, 0.0010, 0.0008] # Adam\n",
    "    #a = [0.25, 0.5, 0.25] # Adadelta\n",
    "    return a[ind] \n",
    "lr = callbacks.LearningRateScheduler(schedule)\n",
    "callbacks_list = [ra_val, checkpoint, early, lr]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135635 samples, validate on 23936 samples\n",
      "Epoch 1/5\n",
      "135552/135635 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9780\n",
      " ROC-AUC - epoch: 1 - score: 0.980673\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04750, saving model to C:/Users/alex_/Google Drive/Kaggle_ToxicComment/weights_base.best.hdf5\n",
      "135635/135635 [==============================] - 877s 6ms/step - loss: 0.0645 - acc: 0.9780 - val_loss: 0.0475 - val_acc: 0.9823\n",
      "Epoch 2/5\n",
      "135552/135635 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9821\n",
      " ROC-AUC - epoch: 2 - score: 0.986057\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04750 to 0.04407, saving model to C:/Users/alex_/Google Drive/Kaggle_ToxicComment/weights_base.best.hdf5\n",
      "135635/135635 [==============================] - 860s 6ms/step - loss: 0.0479 - acc: 0.9821 - val_loss: 0.0441 - val_acc: 0.9829\n",
      "Epoch 3/5\n",
      "135552/135635 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9833\n",
      " ROC-AUC - epoch: 3 - score: 0.987035\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04407 to 0.04257, saving model to C:/Users/alex_/Google Drive/Kaggle_ToxicComment/weights_base.best.hdf5\n",
      "135635/135635 [==============================] - 869s 6ms/step - loss: 0.0440 - acc: 0.9833 - val_loss: 0.0426 - val_acc: 0.9833\n",
      "Epoch 4/5\n",
      "135552/135635 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9836\n",
      " ROC-AUC - epoch: 4 - score: 0.987054\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "135635/135635 [==============================] - 866s 6ms/step - loss: 0.0429 - acc: 0.9836 - val_loss: 0.0430 - val_acc: 0.9831\n",
      "Epoch 5/5\n",
      "135552/135635 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9842\n",
      " ROC-AUC - epoch: 5 - score: 0.987031\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "135635/135635 [==============================] - 877s 6ms/step - loss: 0.0401 - acc: 0.9842 - val_loss: 0.0426 - val_acc: 0.9831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dabaa74d68>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain, ytrain, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          validation_data=(xval, yval),\n",
    "          callbacks = callbacks_list,\n",
    "          verbose=1,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Assessment - Predict on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting....\n",
      "23936/23936 [==============================] - 4s 187us/step\n"
     ]
    }
   ],
   "source": [
    "model.save(path + '\\\\Emb_RNN_CNN.h5')\n",
    "#model.load_weights(filepath)\n",
    "print('Predicting....')\n",
    "ypred = model.predict(xval,batch_size=1024,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the file\n",
    "df_subm = pd.DataFrame(ypred)\n",
    "df_subm.columns = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "# Export to a csv \n",
    "df_subm.to_csv(path + '\\\\submission_2\\\\val_wordembeddings_RNN_CNN.csv', index=True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### % of comments well categorized for all the categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.44886363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex_\\Anaconda3\\envs\\py36\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from utils import unique_category\n",
    "\n",
    "ypred_cat = np.where((ypred >= 0.5), 1, 0) \n",
    "\n",
    "YCompare = np.hstack([yval, ypred])\n",
    "\n",
    "TotalAccuracy = float(np.sum(yval == ypred_cat, axis=0)[0])/len(yval) * 100\n",
    "print(TotalAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the category : toxic\n",
      "Accuracy : 96.44886363636364\n",
      "[[21231   403]\n",
      " [  447  1855]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98     21634\n",
      "          1       0.82      0.81      0.81      2302\n",
      "\n",
      "avg / total       0.96      0.96      0.96     23936\n",
      "\n",
      "Name of the category : severe_toxic\n",
      "Accuracy : 98.9346590909091\n",
      "[[23629    45]\n",
      " [  210    52]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     23674\n",
      "          1       0.54      0.20      0.29       262\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23936\n",
      "\n",
      "Name of the category : obscene\n",
      "Accuracy : 98.19100935828877\n",
      "[[22391   233]\n",
      " [  200  1112]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99     22624\n",
      "          1       0.83      0.85      0.84      1312\n",
      "\n",
      "avg / total       0.98      0.98      0.98     23936\n",
      "\n",
      "Name of the category : threat\n",
      "Accuracy : 99.7033756684492\n",
      "[[23847    11]\n",
      " [   60    18]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     23858\n",
      "          1       0.62      0.23      0.34        78\n",
      "\n",
      "avg / total       1.00      1.00      1.00     23936\n",
      "\n",
      "Name of the category : insult\n",
      "Accuracy : 97.32202540106952\n",
      "[[22413   297]\n",
      " [  344   882]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99     22710\n",
      "          1       0.75      0.72      0.73      1226\n",
      "\n",
      "avg / total       0.97      0.97      0.97     23936\n",
      "\n",
      "Name of the category : identity_hate\n",
      "Accuracy : 99.28141711229947\n",
      "[[23706    10]\n",
      " [  162    58]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     23716\n",
      "          1       0.85      0.26      0.40       220\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23936\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keep columns reffering to categories\n",
    "arr_colnames = train.columns.values.tolist()[2:8]\n",
    "\n",
    "unique_category('toxic', arr_colnames, yval, ypred_cat)\n",
    "unique_category('severe_toxic', arr_colnames, yval, ypred_cat)\n",
    "unique_category('obscene', arr_colnames, yval, ypred_cat)\n",
    "unique_category('threat', arr_colnames, yval, ypred_cat)\n",
    "unique_category('insult', arr_colnames, yval, ypred_cat)\n",
    "unique_category('identity_hate', arr_colnames, yval, ypred_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results - test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 28s 183us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model.load_weights(path + '\\\\Emb_RNN_CNN.h5')\n",
    "#print('Predicting....')\n",
    "y_test_pred = model.predict(x_test,batch_size=1024,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the file\n",
    "df_subm = pd.DataFrame(y_test_pred, index=test.id)\n",
    "df_subm.columns = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "# Export to a csv \n",
    "df_subm.to_csv(path + '\\\\submission_2\\\\wordembeddings_RNN_CNN.csv', index=True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - Words and Character gram + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the comment : <br>\n",
    "* Vectorize the words\n",
    "* Vectorize the characters\n",
    "* Append the 2 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "#print(all_text[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=25000)\n",
    "\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 5),\n",
    "    max_features=35000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fit the model and Predict on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "target = train[class_names]\n",
    "validation = pd.DataFrame(train['id'][idx_val])\n",
    "\n",
    "#idx_train = np.reshape(idx_train, (len(idx_train), 1))\n",
    "#idx_val = np.reshape(idx_val, (len(idx_val), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex_\\Anaconda3\\envs\\py36\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the category : toxic \n",
      "Accuracy : 96.39455213903744\n",
      "[[21485   149]\n",
      " [  714  1588]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98     21634\n",
      "          1       0.91      0.69      0.79      2302\n",
      "\n",
      "avg / total       0.96      0.96      0.96     23936\n",
      "\n",
      "Name of the category : severe_toxic \n",
      "Accuracy : 98.9346590909091\n",
      "[[23612    62]\n",
      " [  193    69]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     23674\n",
      "          1       0.53      0.26      0.35       262\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23936\n",
      "\n",
      "Name of the category : obscene \n",
      "Accuracy : 98.03225267379679\n",
      "[[22529    95]\n",
      " [  376   936]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99     22624\n",
      "          1       0.91      0.71      0.80      1312\n",
      "\n",
      "avg / total       0.98      0.98      0.98     23936\n",
      "\n",
      "Name of the category : threat \n",
      "Accuracy : 99.70755347593582\n",
      "[[23848    10]\n",
      " [   60    18]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     23858\n",
      "          1       0.64      0.23      0.34        78\n",
      "\n",
      "avg / total       1.00      1.00      1.00     23936\n",
      "\n",
      "Name of the category : insult \n",
      "Accuracy : 97.23011363636364\n",
      "[[22541   169]\n",
      " [  494   732]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99     22710\n",
      "          1       0.81      0.60      0.69      1226\n",
      "\n",
      "avg / total       0.97      0.97      0.97     23936\n",
      "\n",
      "Name of the category : identity_hate \n",
      "Accuracy : 99.24381684491979\n",
      "[[23696    20]\n",
      " [  161    59]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     23716\n",
      "          1       0.75      0.27      0.39       220\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23936\n",
      "\n",
      "                      id     toxic  severe_toxic   obscene    threat  \\\n",
      "90462   f2127de2c6ebb20d  0.002516      0.000328  0.001801  0.000495   \n",
      "74993   c8a439c06a2a4ee1  0.020877      0.001990  0.003746  0.000760   \n",
      "49549   847b005c029b4c14  0.008763      0.001637  0.002502  0.001057   \n",
      "139942  ecfa58c45befb35b  0.009199      0.000526  0.002624  0.000328   \n",
      "74342   c6e19281981ff26d  0.016264      0.004972  0.008465  0.002705   \n",
      "71110   be53d3dab7c8549f  0.007960      0.000710  0.005495  0.000812   \n",
      "30318   5081d65e63644d25  0.014379      0.001103  0.004837  0.000976   \n",
      "15144   27fc84ebf94d89a6  0.993631      0.004420  0.437261  0.004062   \n",
      "86154   e67702b540a84b6e  0.001247      0.000737  0.004013  0.000592   \n",
      "\n",
      "          insult  identity_hate  \n",
      "90462   0.000995       0.000422  \n",
      "74993   0.005682       0.001679  \n",
      "49549   0.003339       0.001460  \n",
      "139942  0.003070       0.000418  \n",
      "74342   0.009569       0.002650  \n",
      "71110   0.003426       0.000979  \n",
      "30318   0.004625       0.001838  \n",
      "15144   0.778991       0.019778  \n",
      "86154   0.001704       0.001311  \n"
     ]
    }
   ],
   "source": [
    "for class_name in class_names:\n",
    "    \n",
    "    target = np.asarray(train[class_name])\n",
    "    target = np.reshape(target, len(target),1)\n",
    "    \n",
    "    ytrain = target[idx_train]\n",
    "    yval = target[idx_val]\n",
    "    \n",
    "    train_features2 = train_features.tocsr()  \n",
    "    xtrain = train_features2[idx_train,:]\n",
    "    xval = train_features2[idx_val,:]\n",
    "    \n",
    "    classifier = LogisticRegression(C=1, solver='sag')\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    validation[class_name] = classifier.predict_proba(xval)[:, 1]\n",
    "    \n",
    "    pred = round(validation[class_name])\n",
    "    pred = np.reshape(pred, (len(pred), 1))\n",
    "    \n",
    "    cm = confusion_matrix(yval, pred)\n",
    "    cr = classification_report(yval, pred)\n",
    "\n",
    "    print('Name of the category : {} '.format(class_name))\n",
    "    print(\"Accuracy : {}\".format(format((float(cm[0,0]) + float(cm[1,1]))/len(yval)*100)))\n",
    "    print(cm)\n",
    "    print(cr)\n",
    "    \n",
    "validation.to_csv(path + '\\\\submission_2\\\\val_logreg_words_chars.csv', index=False, header = True)\n",
    "print(validation[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.39455213903744\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from utils import unique_category\n",
    "\n",
    "target = np.asarray(train[class_names])[idx_val]\n",
    "ypred_cat = np.where((validation[class_names] >= 0.5), 1, 0) \n",
    "\n",
    "TotalAccuracy = float(np.sum(target == ypred_cat, axis=0)[0])/len(yval) * 100\n",
    "print(TotalAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict  on test set + export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    classifier.fit(train_features, train_target)\n",
    "    submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "    print('Class \"{}\" done'.format(class_name))\n",
    "    \n",
    "print(submission[0:9])\n",
    "submission.to_csv(path + '\\\\submission_2\\\\logreg_words_chars.csv', index=False, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - Word Embeddings + DeepCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization,PReLU\n",
    "from keras.layers.merge import add, Concatenate\n",
    "\n",
    "max_features=150000\n",
    "maxlen=200\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 200, 300)     45000000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 200, 300)     0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 200, 64)      57664       spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200, 64)      256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 200, 64)      12800       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 200, 64)      12352       p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200, 64)      256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 200, 64)      19264       spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, 200, 64)      12800       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_3 (PReLU)               (None, 200, 64)      12800       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 200, 64)      0           p_re_lu_2[0][0]                  \n",
      "                                                                 p_re_lu_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 99, 64)       0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 99, 64)       12352       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 99, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_4 (PReLU)               (None, 99, 64)       6336        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 99, 64)       12352       p_re_lu_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 99, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_5 (PReLU)               (None, 99, 64)       6336        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 99, 64)       0           p_re_lu_5[0][0]                  \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 49, 64)       0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 49, 64)       12352       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 49, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_6 (PReLU)               (None, 49, 64)       3136        batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 49, 64)       12352       p_re_lu_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 49, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_7 (PReLU)               (None, 49, 64)       3136        batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 49, 64)       0           p_re_lu_7[0][0]                  \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 24, 64)       0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 24, 64)       12352       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 24, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_8 (PReLU)               (None, 24, 64)       1536        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 24, 64)       12352       p_re_lu_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 24, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_9 (PReLU)               (None, 24, 64)       1536        batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 24, 64)       0           p_re_lu_9[0][0]                  \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 64)           0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          16640       global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256)          1024        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_10 (PReLU)              (None, 256)          256         batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           p_re_lu_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 6)            1542        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 45,245,318\n",
      "Trainable params: 243,782\n",
      "Non-trainable params: 45,001,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(maxlen, ))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "\n",
    "def dpcnn(embedded_sequences):\n",
    "    filter_nr = 64\n",
    "    filter_size = 3\n",
    "    max_pool_size = 3\n",
    "    max_pool_strides = 2\n",
    "    dense_nr = 256\n",
    "    dense_dropout = 0.5\n",
    "    \n",
    "    block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(embedded_sequences)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = PReLU()(block1)\n",
    "    block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = PReLU()(block1)\n",
    "    \n",
    "    #we pass embedded comment through conv1d with filter size 1 because it needs to have the same shape as block output\n",
    "    #if you choose filter_nr = embed_size (300 in this case) you don't have to do this part and can add emb_comment directly to block1_output\n",
    "    resize_emb = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(embedded_sequences)\n",
    "    resize_emb = PReLU()(resize_emb)\n",
    "        \n",
    "    block1_output = add([block1, resize_emb])\n",
    "    block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block1_output)\n",
    "    \n",
    "    block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block1_output)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = PReLU()(block2)\n",
    "    block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block2)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = PReLU()(block2)\n",
    "        \n",
    "    block2_output = add([block2, block1_output])\n",
    "    block2_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block2_output)\n",
    "    \n",
    "    block3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block2_output)\n",
    "    block3 = BatchNormalization()(block3)\n",
    "    block3 = PReLU()(block3)\n",
    "    block3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block3)\n",
    "    block3 = BatchNormalization()(block3)\n",
    "    block3 = PReLU()(block3)\n",
    "        \n",
    "    block3_output = add([block3, block2_output])\n",
    "    block3_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block3_output)\n",
    "    \n",
    "    block4 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block3_output)\n",
    "    block4 = BatchNormalization()(block4)\n",
    "    block4 = PReLU()(block4)\n",
    "    block4 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block4)\n",
    "    block4 = BatchNormalization()(block4)\n",
    "    block4 = PReLU()(block4)\n",
    "    \n",
    "    output = add([block4, block3_output])\n",
    "    output = GlobalMaxPooling1D()(output)\n",
    "    \n",
    "    output = Dense(dense_nr, activation='linear')(output)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = PReLU()(output)\n",
    "    output = Dropout(dense_dropout)(output)\n",
    "    output = Dense(6, activation='sigmoid')(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "output = dpcnn(x)\n",
    "model_dpcnn = Model(inputs=sequence_input, outputs=output)\n",
    "model_dpcnn.compile(loss='binary_crossentropy',\n",
    "                  optimizer='Adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_dpcnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "#xtrain, xval, ytrain, yval = train_test_split(x_train, y_train, train_size=0.85, random_state=17)\n",
    "\n",
    "xtrain = x_train[idx_train]\n",
    "xval = x_train[idx_val]\n",
    "ytrain = y_train[idx_train]\n",
    "yval = y_train[idx_val]\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "ra_val = RocAucEvaluation(validation_data=(xval, yval), interval = 1)\n",
    "def schedule(ind):\n",
    "    a = [0.0008, 0.0010, 0.0008, 0.0010, 0.0008] # Adam\n",
    "    #a = [0.25, 0.5, 0.25] # Adadelta\n",
    "    return a[ind] \n",
    "lr = callbacks.LearningRateScheduler(schedule)\n",
    "callbacks_list = [ra_val, checkpoint, early, lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135635 samples, validate on 23936 samples\n",
      "Epoch 1/5\n",
      "135424/135635 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9734- ETA: 0s - loss: 0.0772 - acc: 0\n",
      " ROC-AUC - epoch: 1 - score: 0.977076\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04795, saving model to C:/Users/alex_/Google Drive/Kaggle_ToxicComment/weights_base.best.hdf5\n",
      "135635/135635 [==============================] - 53s 393us/step - loss: 0.0770 - acc: 0.9734 - val_loss: 0.0480 - val_acc: 0.9818\n",
      "Epoch 2/5\n",
      "135552/135635 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9818\n",
      " ROC-AUC - epoch: 2 - score: 0.983659\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04795 to 0.04565, saving model to C:/Users/alex_/Google Drive/Kaggle_ToxicComment/weights_base.best.hdf5\n",
      "135635/135635 [==============================] - 51s 377us/step - loss: 0.0496 - acc: 0.9818 - val_loss: 0.0457 - val_acc: 0.9825\n",
      "Epoch 3/5\n",
      "135552/135635 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9827\n",
      " ROC-AUC - epoch: 3 - score: 0.986566\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04565 to 0.04471, saving model to C:/Users/alex_/Google Drive/Kaggle_ToxicComment/weights_base.best.hdf5\n",
      "135635/135635 [==============================] - 52s 386us/step - loss: 0.0454 - acc: 0.9827 - val_loss: 0.0447 - val_acc: 0.9827\n",
      "Epoch 4/5\n",
      "135552/135635 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9831\n",
      " ROC-AUC - epoch: 4 - score: 0.984780\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "135635/135635 [==============================] - 51s 375us/step - loss: 0.0435 - acc: 0.9831 - val_loss: 0.0463 - val_acc: 0.9822\n",
      "Epoch 5/5\n",
      "135552/135635 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9839\n",
      " ROC-AUC - epoch: 5 - score: 0.987077\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04471 to 0.04270, saving model to C:/Users/alex_/Google Drive/Kaggle_ToxicComment/weights_base.best.hdf5\n",
      "135635/135635 [==============================] - 51s 377us/step - loss: 0.0409 - acc: 0.9839 - val_loss: 0.0427 - val_acc: 0.9831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dab431da58>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dpcnn.fit(xtrain, ytrain, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          validation_data=(xval, yval),\n",
    "          callbacks = callbacks_list,\n",
    "          verbose=1,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting....\n",
      "23936/23936 [==============================] - 1s 34us/step\n"
     ]
    }
   ],
   "source": [
    "model_dpcnn.save(path + '\\\\Emb_dpcnn.h5')\n",
    "model_dpcnn.load_weights(path + '\\\\Emb_dpcnn.h5')\n",
    "print('Predicting....')\n",
    "ypred = model_dpcnn.predict(xval,batch_size=1024,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the file\n",
    "df_subm = pd.DataFrame(ypred)\n",
    "df_subm.columns = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "# Export to a csv \n",
    "df_subm.to_csv(path + '\\\\submission_2\\\\val_wordembeddings_dpCNN.csv', index=True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### % of comments well categorized for all the categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.44468582887701\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from utils import unique_category\n",
    "\n",
    "ypred_cat = np.where((ypred >= 0.5), 1, 0) \n",
    "\n",
    "YCompare = np.hstack([yval, ypred])\n",
    "\n",
    "TotalAccuracy = float(np.sum(yval == ypred_cat, axis=0)[0])/len(yval) * 100\n",
    "print(TotalAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the category : toxic\n",
      "Accuracy : 96.44468582887701\n",
      "[[21327   307]\n",
      " [  544  1758]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98     21634\n",
      "          1       0.85      0.76      0.81      2302\n",
      "\n",
      "avg / total       0.96      0.96      0.96     23936\n",
      "\n",
      "Name of the category : severe_toxic\n",
      "Accuracy : 98.91377005347593\n",
      "[[23612    62]\n",
      " [  198    64]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     23674\n",
      "          1       0.51      0.24      0.33       262\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23936\n",
      "\n",
      "Name of the category : obscene\n",
      "Accuracy : 98.18683155080214\n",
      "[[22402   222]\n",
      " [  212  1100]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99     22624\n",
      "          1       0.83      0.84      0.84      1312\n",
      "\n",
      "avg / total       0.98      0.98      0.98     23936\n",
      "\n",
      "Name of the category : threat\n",
      "Accuracy : 99.72426470588235\n",
      "[[23851     7]\n",
      " [   59    19]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     23858\n",
      "          1       0.73      0.24      0.37        78\n",
      "\n",
      "avg / total       1.00      1.00      1.00     23936\n",
      "\n",
      "Name of the category : insult\n",
      "Accuracy : 97.34291443850267\n",
      "[[22387   323]\n",
      " [  313   913]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99     22710\n",
      "          1       0.74      0.74      0.74      1226\n",
      "\n",
      "avg / total       0.97      0.97      0.97     23936\n",
      "\n",
      "Name of the category : identity_hate\n",
      "Accuracy : 99.2730614973262\n",
      "[[23692    24]\n",
      " [  150    70]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     23716\n",
      "          1       0.74      0.32      0.45       220\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23936\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keep columns reffering to categories\n",
    "arr_colnames = train.columns.values.tolist()[2:8]\n",
    "\n",
    "unique_category('toxic', arr_colnames, yval, ypred_cat)\n",
    "unique_category('severe_toxic', arr_colnames, yval, ypred_cat)\n",
    "unique_category('obscene', arr_colnames, yval, ypred_cat)\n",
    "unique_category('threat', arr_colnames, yval, ypred_cat)\n",
    "unique_category('insult', arr_colnames, yval, ypred_cat)\n",
    "unique_category('identity_hate', arr_colnames, yval, ypred_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Export results - test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 6s 38us/step\n"
     ]
    }
   ],
   "source": [
    "model_dpcnn.load_weights(path + '\\\\Emb_dpcnn.h5')\n",
    "y_test_pred = model_dpcnn.predict(x_test,batch_size=1024,verbose=1)\n",
    "\n",
    "# Prepare the file\n",
    "df_subm = pd.DataFrame(y_test_pred, index=test.id)\n",
    "df_subm.columns = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "# Export to a csv \n",
    "df_subm.to_csv(path + '\\\\submission_2\\\\wordembeddings_dpCNN.csv', index=True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "path = \"C:/Users/alex_/Google Drive/Kaggle_ToxicComment/\"\n",
    "sys.path.append(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_logreg = pd.read_csv(path + \"/submission_2/val_logreg_words_chars.csv\",\n",
    "skipinitialspace=True,\n",
    "header = 0,\n",
    "sep = ',',\n",
    "encoding='utf8')\n",
    "\n",
    "wordembedding_dpcnn = pd.read_csv(path + \"/submission_2/val_wordembeddings_dpcnn.csv\",\n",
    "skipinitialspace=True,\n",
    "header = 0,\n",
    "sep = ',',\n",
    "encoding='utf8')\n",
    " \n",
    "wordembedding_gru_cnn = pd.read_csv(path + \"/submission_2/val_wordembeddings_RNN_CNN.csv\",\n",
    "skipinitialspace=True,\n",
    "header = 0,\n",
    "sep = ',',\n",
    "encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
      "       'identity_hate'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(23936, 6)\n",
      "(23936, 6)\n"
     ]
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "model_ens = simple_logreg.copy()\n",
    "\n",
    "model_ens[label_cols] = (simple_logreg[label_cols] +\n",
    "                         wordembedding_dpcnn[label_cols] + \n",
    "                         wordembedding_gru_cnn[label_cols]) / 3\n",
    "\n",
    "model_ens = model_ens[label_cols]\n",
    "\n",
    "print(model_ens.columns)\n",
    "print(type(model_ens))\n",
    "yval = y_train[idx_val]\n",
    "print(model_ens.shape)\n",
    "print(yval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.75802139037432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from utils import unique_category\n",
    "\n",
    "ypred_cat = np.where((model_ens >= 0.5), 1, 0) \n",
    "\n",
    "YCompare = np.hstack([yval, ypred])\n",
    "\n",
    "TotalAccuracy = float(np.sum(yval == ypred_cat, axis=0)[0])/len(yval) * 100\n",
    "print(TotalAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the category : toxic\n",
      "Accuracy : 96.75802139037432\n",
      "[[21404   230]\n",
      " [  546  1756]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98     21634\n",
      "          1       0.88      0.76      0.82      2302\n",
      "\n",
      "avg / total       0.97      0.97      0.97     23936\n",
      "\n",
      "Name of the category : severe_toxic\n",
      "Accuracy : 98.96808155080214\n",
      "[[23628    46]\n",
      " [  201    61]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     23674\n",
      "          1       0.57      0.23      0.33       262\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23936\n",
      "\n",
      "Name of the category : obscene\n",
      "Accuracy : 98.32052139037432\n",
      "[[22469   155]\n",
      " [  247  1065]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99     22624\n",
      "          1       0.87      0.81      0.84      1312\n",
      "\n",
      "avg / total       0.98      0.98      0.98     23936\n",
      "\n",
      "Name of the category : threat\n",
      "Accuracy : 99.73262032085562\n",
      "[[23854     4]\n",
      " [   60    18]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     23858\n",
      "          1       0.82      0.23      0.36        78\n",
      "\n",
      "avg / total       1.00      1.00      1.00     23936\n",
      "\n",
      "Name of the category : insult\n",
      "Accuracy : 97.45571524064172\n",
      "[[22488   222]\n",
      " [  387   839]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99     22710\n",
      "          1       0.79      0.68      0.73      1226\n",
      "\n",
      "avg / total       0.97      0.97      0.97     23936\n",
      "\n",
      "Name of the category : identity_hate\n",
      "Accuracy : 99.2730614973262\n",
      "[[23705    11]\n",
      " [  163    57]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     23716\n",
      "          1       0.84      0.26      0.40       220\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23936\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keep columns reffering to categories\n",
    "arr_colnames = train.columns.values.tolist()[2:8]\n",
    "\n",
    "unique_category('toxic', arr_colnames, yval, ypred_cat)\n",
    "unique_category('severe_toxic', arr_colnames, yval, ypred_cat)\n",
    "unique_category('obscene', arr_colnames, yval, ypred_cat)\n",
    "unique_category('threat', arr_colnames, yval, ypred_cat)\n",
    "unique_category('insult', arr_colnames, yval, ypred_cat)\n",
    "unique_category('identity_hate', arr_colnames, yval, ypred_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_logreg = pd.read_csv(path + \"/submission_2/logreg_words_chars.csv\",\n",
    "skipinitialspace=True,\n",
    "header = 0,\n",
    "sep = ',',\n",
    "encoding='utf8')\n",
    "\n",
    "wordembedding_dpcnn = pd.read_csv(path + \"/submission_2/wordembeddings_dpcnn.csv\",\n",
    "skipinitialspace=True,\n",
    "header = 0,\n",
    "sep = ',',\n",
    "encoding='utf8')\n",
    " \n",
    "wordembedding_gru_cnn = pd.read_csv(path + \"/submission_2/wordembeddings_RNN_CNN.csv\",\n",
    "skipinitialspace=True,\n",
    "header = 0,\n",
    "sep = ',',\n",
    "encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "model_ens = simple_logreg.copy()\n",
    "model_ens[label_cols] = (simple_logreg[label_cols] +\n",
    "                         wordembedding_dpcnn[label_cols] + \n",
    "                         wordembedding_gru_cnn[label_cols]) / 3\n",
    "\n",
    "df_subm = pd.DataFrame(model_ens)\n",
    "df_subm = df_subm[['id', 'toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "df_subm.to_csv(path + '\\\\submission_2\\\\model_ensemble_1.csv', index=False, header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
